{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [CPSC 322](https://github.com/GonzagaCPSC322) Data Science Algorithms\n",
    "[Gonzaga University](https://www.gonzaga.edu/)\n",
    "\n",
    "[Gina Sprint](http://cs.gonzaga.edu/faculty/sprint/)\n",
    "\n",
    "# Attribute Selection\n",
    "What are our learning objectives for this lesson?\n",
    "* Use entropy to select attributes to split on\n",
    "* Create decision trees with continuous attributes\n",
    "\n",
    "Content used in this lesson is based upon information in the following sources:\n",
    "* Dr. Shawn Bowers' Data Mining notes\n",
    "* [Data Science from Scratch](https://www.amazon.com/Data-Science-Scratch-Principles-Python/dp/149190142X/ref=sr_1_1?ie=UTF8&qid=1491521130&sr=8-1&keywords=joel+grus) by Joel Grus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warm-up Task(s)\n",
    "Work on where we left off on Intro to Decision Trees Lab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Today\n",
    "* Announcements\n",
    "    * Thank you to everyone who came to the LLM Workshop last night, we had a great turn out!!\n",
    "    * PA5 is due tonight. Questions?\n",
    "        * Note: Python is pass by object reference (this is important for `train_test_split()`)\n",
    "    * Work on PA6\n",
    "        * Let's go over it\n",
    "        * Note: you don't need attribute names in `fit()` because Naive Bayes can use general names like \"att0\", \"att1\", etc.\n",
    "    * IQ7 on Thursday on PA5 topics\n",
    "    * MA9 is posted\n",
    "* Today\n",
    "    * Finish Intro to Decision Trees Lab\n",
    "    * Entropy Lab\n",
    "    * (if time) Decision trees (AKA PA7) starter code!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Review) Lab Task 1\n",
    "Consider the following dataset. In this dataset, each instance example is an attribute list describing a job candidate:\n",
    "    \n",
    "|level|lang|tweets|phd|interviewed_well|\n",
    "|-|-|-|-|-|\n",
    "|Senior|Java|no|no|False|\n",
    "|Senior|Java|no|yes|False|\n",
    "|Mid|Python|no|no|True|\n",
    "|Junior|Python|no|no|True|\n",
    "|Junior|R|yes|no|True|\n",
    "|Junior|R|yes|yes|False|\n",
    "|Mid|R|yes|yes|True|\n",
    "|Senior|Python|no|no|False|\n",
    "|Senior|R|yes|no|True|\n",
    "|Junior|Python|yes|no|True|\n",
    "|Senior|Python|yes|yes|True|\n",
    "|Mid|Python|no|yes|True|\n",
    "|Mid|Java|yes|no|True|\n",
    "|Junior|Python|no|yes|False|\n",
    "\n",
    "Here are the attribute domains (possible values for an attribute):\n",
    "* Level of expertise (string): Junior, Mid, Senior\n",
    "* Preferred language (string): Java, Python, R\n",
    "* Whether she is active on twitter (boolean): yes, no\n",
    "* Whether she has a PhD (boolean): yes, no\n",
    "* CLASS: Interviewed well? (boolean): True, False\n",
    "\n",
    "Construct a decision tree that first splits on `level`. For the `Senior` partition, split on `tweets`. For the `Junior` partition, split on `phd`. Label each of your leaf nodes with the class proportion of the partition of that subtree.\n",
    "\n",
    "### (Review) Lab Task 2\n",
    "Use your tree from the previous task to classify the following test instances:\n",
    "1. $X_{1}$ = `[\"Junior\", \"Java\", \"yes\", \"no\"]`\n",
    "1. $X_{2}$ = `[\"Junior\", \"Java\", \"yes\", \"yes\"]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting Attributes\n",
    "Resulting decision tree depends on attribute selection approach\n",
    "* Want high predictive accuracy with small number of rules\n",
    "* In practice, using **\"information gain\"** does well (popular approach)\n",
    "\n",
    "Basic idea:\n",
    "* Select attribute with the largest \"information gain\"\n",
    "* Typically the attribute that most \"unevenly\" partitions the instances on class\n",
    "\n",
    "How it works: use \"Shannon Entropy\" as a measure of information content\n",
    "* Q: how many bits are needed to represent numbers between 1 and 64?\n",
    "    * $log_2 64 = 6$ bits\n",
    "* What if instead we had messages involving combinations of 64 words\n",
    "    * Could capture each word using a 6 bit number\n",
    "    * Thus a message with 10 words would cost 10 Ã— 6 = 60 bits\n",
    "* However, if we knew more about the distribution of words, we could (on average) use fewer bits per message!\n",
    "    * e.g., \"the\" occurs more than any other word\n",
    "    * Use encodings whose lengths are inversely proportional to their frequencies (probabilities)\n",
    "* Entropy gives a precise lower bound for expected number of bits needed to encode a message (based on a probability distribution)\n",
    "\n",
    "## Entropy $E$\n",
    "The details:\n",
    "* Entropy $E = 0$ implies low content (e.g., all values are the same)\n",
    "* Highest entropy value when all values equally likely (most content)\n",
    "* Entropy formula assumes information encoded in bits ... (thus, $log_2$)\n",
    "$$E = -\\sum_{i=1}^{n}p_i log_2(p_i)$$\n",
    "    * $n$ for us is the number of class labels\n",
    "    * $p_i$ is proportion of instances labeled with class $i$ (i.e., $P(C_{i})$)\n",
    "    * Note $p_i$ is assumed to be strictly greater than 0, up to and including 1\n",
    "* What the formula is saying:\n",
    "    * Since $0 < p_i \\leq 1$, we know that $-p_i log_2(p_i) \\geq 0$ is positive\n",
    "    * e.g., for $log_2(0.5) = y$, we have $2^y = \\frac{1}{2}$, which means $y = -1$\n",
    "    * If $p_i = 1$, then $-p_i log_2(p_i) = 0$\n",
    "    * $E$ has the highest value when labels are equally distributed\n",
    "    \n",
    "The function $-p_i log_2(p_i)$ for $0 < p_i \\leq 1$\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/GonzagaCPSC322/U5-Decision-Trees/master/figures/entropy_graph.png\" width=\"500\"/>\n",
    "\n",
    "The function $-(p_i log_2(p_i) + (1-p_i) log_2 (1 - p_i))$\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/GonzagaCPSC322/U5-Decision-Trees/master/figures/entropy_binary_classification_graph.png\" width=\"500\"/>\n",
    "\n",
    "* This mimics having just two labels\n",
    "* As shown, the closer the two labels the higher the entropy\n",
    "\n",
    "Pick the attribute that maximizes information gain\n",
    "* Information Gain = $E_{start} - E_{new}$\n",
    "    * At each partition, pick attribute with highest information gain\n",
    "    * That is, split on attribute with greatest reduction in entropy\n",
    "    * Which means find attribute with smallest $E_{new}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab Task 3\n",
    "What is $E$ for the following distributions? Recall: $E = -\\sum_{i=1}^{n}p_i log_2(p_i)$\n",
    "1. $p_{yes} = 3/8$ and $p_{no} = 5/8$\n",
    "1. $p_{yes} = 2/8$ and $p_{no} = 6/8$\n",
    "    1. Before you calculate this, will $E$ for this distribution be larger or smaller? \n",
    "        1. ... should be smaller!\n",
    "\n",
    "Solutions in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9544340029249649\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "# 1.1\n",
    "p_yes = 3 / 8\n",
    "p_no = 5 / 8\n",
    "E = -(p_yes * math.log(p_yes, 2) + p_no * math.log(p_no, 2))\n",
    "print(E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8112781244591328\n"
     ]
    }
   ],
   "source": [
    "# 1. 2\n",
    "p_yes = 2 / 8\n",
    "p_no = 6 / 8\n",
    "E = -(p_yes * math.log(p_yes, 2) + p_no * math.log(p_no, 2))\n",
    "print(E)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab Task 4\n",
    "Consider the following dataset. In this dataset, each instance example is an attribute list describing a job candidate:\n",
    "    \n",
    "|level|lang|tweets|phd|interviewed_well|\n",
    "|-|-|-|-|-|\n",
    "|Senior|Java|no|no|False|\n",
    "|Senior|Java|no|yes|False|\n",
    "|Mid|Python|no|no|True|\n",
    "|Junior|Python|no|no|True|\n",
    "|Junior|R|yes|no|True|\n",
    "|Junior|R|yes|yes|False|\n",
    "|Mid|R|yes|yes|True|\n",
    "|Senior|Python|no|no|False|\n",
    "|Senior|R|yes|no|True|\n",
    "|Junior|Python|yes|no|True|\n",
    "|Senior|Python|yes|yes|True|\n",
    "|Mid|Python|no|yes|True|\n",
    "|Mid|Java|yes|no|True|\n",
    "|Junior|Python|no|yes|False|\n",
    "\n",
    "Compute $E_{start}$ for the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating $E_{new}$ for an Attribute\n",
    "Assume we want to partition $D$ on an attribute $A$\n",
    "* Where $A$ has values $a_1, a_2, ... , a_v$\n",
    "* Creating partitions $D_1, D_2, ... , D_v$\n",
    "\n",
    "We'd like each partition to be \"pure\" (contain instances of same class label)\n",
    "* They may not be, however (i.e., they may have \"clashes\")\n",
    "* The amount of additional info still needed for a \"pure\" classification is:\n",
    "$$E_{new} = \\sum_{j=1}^{v} \\frac{|D_j|}{|D|} \\times E_{D_j}$$\n",
    "\n",
    "* where $E_{D_j}$ is the entropy of partition $D_j$\n",
    "* $\\frac{|D_j|}{|D|}$ is the likelihood an instance is in the $j$-th partition\n",
    "* Thus, $E_{new}$ is a **weighted average** of the attributes corresponding partitions' entropies\n",
    "\n",
    "Information Gain = $E_{start}$ - $E_{new}$\n",
    "* We want to maximize information gain, meaning we want to pick the attribute with the smallest $E_{new}$ value\n",
    "* This means the smallest amount of information needed to classify an instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab Task 5\n",
    "For the interview dataset, we need to figure out which attribute to split on first. We need to compute $E_{new} = \\sum_{j=1}^{v} \\frac{|D_j|}{|D|} \\times E_{D_j}$, where $E_{D_j}$ is the entropy of partition $D_j$ and $\\frac{|D_j|}{|D|}$ is the likelihood an instance is in the $j$-th partition for each attribute. Then choose the attribute with the smallest $E_{new}$ (e.g. has the largest $Information Gain = E_{start} - E_{new}$). \n",
    "\n",
    "Let's start by partitioning on the level attribute by computing $E_{new(level)}$.\n",
    "1. Compute $E_{Senior}$\n",
    "1. Compute $E_{Mid}$\n",
    "1. Compute $E_{Junior}$\n",
    "1. Compute $E_{new(level)}$\n",
    "1. Compute $Information Gain_{level} = E_{start} - E_{new(level)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab Task 6\n",
    "Compute $E_{new}$ for the other three attributes.\n",
    "1. Partition on lang by computing $Information Gain_{lang} = E_{start} - E_{new(lang)}$\n",
    "1. Partition on tweets by computing $Information Gain_{tweets} = E_{start} - E_{new(tweets)}$\n",
    "1. Partition on phd by computing $Information Gain_{phd} = E_{start} - E_{new(phd)}$\n",
    "\n",
    "Which attribute should we split on first? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab Task 7 (For Additional Entropy Practice)\n",
    "Using entropy for attribute selection, finish Bramer 5.5 Self-assessment exercise 1 to produce a decision tree for the *degrees* dataset. Note that Bramer's tree in  Figure 4.4 is NOT the entropy-based solution for this dataset, you will need to finish the exercise to construct the correct tree.\n",
    "\n",
    "### Lab Task 8 (For Additional Entropy Practice)\n",
    "|standing |job_status |credit_rating |buys_iphone|\n",
    "|-|-|-|-|\n",
    "|1 |3 |fair |no|\n",
    "|1 |3 |excellent |no|\n",
    "|2 |3 |fair |yes|\n",
    "|2 |2 |fair |yes|\n",
    "|2 |1 |fair |yes|\n",
    "|2 |1 |excellent |no|\n",
    "|2 |1 |excellent |yes|\n",
    "|1 |2 |fair |no|\n",
    "|1 |1 |fair |yes|\n",
    "|2 |2 |fair |yes|\n",
    "|1 |2 |excellent |yes|\n",
    "|2 |2 |excellent |yes|\n",
    "|2 |3 |fair |yes|\n",
    "|2 |2 |excellent |no|\n",
    "|2 |3 |fair |yes|\n",
    "\n",
    "Using entropy for attribute selection, produce a decision tree for the iphone dataset. Hint: figure out which attribute to split on first:\n",
    "1. Calculate $E_{new}$ for `standing` for the whole table\n",
    "1. Calculate $E_{new}$ for `job_status` for the whole table\n",
    "1. Calculate $E_{new}$ for `credit_rating` for the whole table\n",
    "\n",
    "Note: this dataset has clashes!! On majority vote ties, choose label alphabetically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with Continuous Attributes\n",
    "* So far we've mainly assumed categorical attributes\n",
    "* Like always we can discretize continuous attributes first\n",
    "* Alternatively, we can use Entropy to find a \"split point\"\n",
    "\n",
    "The random approach:\n",
    "* Pick a random value $v$ from the set of values in the attribute\n",
    "* Use $v$ as a \"split point\"\n",
    "* i.e., divide into two partitions: $\\leq v$ and $> v$\n",
    "\n",
    "Using entropy instead:\n",
    "1. Sort the values in ascending order $[v_1, v_2, ... , v_k]$\n",
    "1. For each split point $v$ in $v_1$ through $v_{kâˆ’1}$ calculate $E_{new}$\n",
    "    * Thus, two partitions in each case ($\\leq v$ and $> v$)\n",
    "    * Alternatively, use half-way points between adjacent values $v_i$ and $v_{i+1}$\n",
    "1. Select the split point that minimizes $E_{new}$"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
