{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [CPSC 322](https://github.com/GonzagaCPSC322) Data Science Algorithms\n",
    "[Gonzaga University](https://www.gonzaga.edu/)\n",
    "\n",
    "[Gina Sprint](http://cs.gonzaga.edu/faculty/sprint/)\n",
    "\n",
    "# Decision Trees\n",
    "What are our learning objectives for this lesson?\n",
    "* Learn decision tree terminology\n",
    "* Introduce the TDIDT algorithm and clashes\n",
    "\n",
    "Content used in this lesson is based upon information in the following sources:\n",
    "* Dr. Shawn Bowers' Data Mining notes\n",
    "* [Data Science from Scratch](https://www.amazon.com/Data-Science-Scratch-Principles-Python/dp/149190142X/ref=sr_1_1?ie=UTF8&qid=1491521130&sr=8-1&keywords=joel+grus) by Joel Grus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warm-up Task(s) 10/22\n",
    "1. Check in with your neighbor, see how their long weekend was :)\n",
    "1. Try lab task #1 on the Classifier Performance Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Today 10/22\n",
    "* Announcements\n",
    "    * Let's go over IQ5\n",
    "    * IQ6 on Thursday on machine learning basics and PA4 topics\n",
    "        * Note: it will be at the start of class because we have a guest speaker coming at the end! Come to class early to get a head start on it!\n",
    "    * PA5 due one week from today\n",
    "        * Demo of Sci-kit Learn functionality we are trying to match: `ClassificationFun/sklearn_training_testing.py`\n",
    "        * Note: See ClassificationFun for a `randomize_in_place()` function you can use. Also, I recommend using `np.random` for your random number generator consistently.\n",
    "        * Questions?\n",
    "    * PA6 (Naive Bayes & classifier performance) is posted\n",
    "        * Algorithms to Live By Chapter Ch.6 Bayes' Rule (posted to our GDdrive Folder) is a great read on how computer science and probability can be applied to everyday life\n",
    "    * No MA due next week :) Work on PA5 & 6\n",
    "    * Career fair on Thursday! (+1 bonus point future quiz for talking to at least 1 company at career fair)\n",
    "        * I'll also do +1 bonus point on future quiz if participate in Hackathon!\n",
    "    * Note on midterm grades: I will be entering 0s for missing MA1-6 & PA1-3\n",
    "* Today\n",
    "    * MA8 quiz\n",
    "    * Classifer Performance Lab\n",
    "    * Intro to Decision Trees Lab\n",
    "    * 9:05am: Gaby from FAST Enterprises\n",
    "        * Interview workshop tomorrow 10/23 at 5:15pm (Bollier 121)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Warm-up Task(s) 10/24\n",
    "None! Let's take IQ6."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Today 10/24\n",
    "* Announcements\n",
    "    * No MA due next week :) Work on PA5 & 6\n",
    "    * PA5 is due on Tuesday. Questions?\n",
    "    * Work on PA6\n",
    "        * Let's go over MA7 solution (you should check your MA7 against this before implementing the iPhone dataset as a `MyNaiveBayesClassifier` test case on PA6)\n",
    "    * Go to the career fair today! (bonus opportunity on future quiz!)\n",
    "    * Come to ACM/WiC's LLM Workshop on Monday 10/28 5:15pm Bollier 120 (bonus opportunity on future quiz!)\n",
    "* IQ6 first ~15 mins of class\n",
    "* Finish Measuring Classifier Performance Lab\n",
    "* Intro to Decision Trees Lab\n",
    "* (if time) Start Entropy Lab\n",
    "* 9:05am: Nick from Schweitzer Engineering Labs (SEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro to Decision Tree Classifiers\n",
    "$k$-NN and Naive Bayes are \"instance-at-a-time\" classifiers\n",
    "* Given a new instance, use training set to predict class label\n",
    "* Hard to know \"why\" or what overall pattern led to prediction\n",
    "* Highly dependent on particular instance given (its attribute values)\n",
    "\n",
    "Decision trees are \"rule\"-based classifiers\n",
    "* Build a set of general rules from training set\n",
    "* Like a \"compiled\" version of the training set\n",
    "* Use rules (not training set) to classify new instances\n",
    "\n",
    "Rules are basic if-then statements:\n",
    "\n",
    ">IF $att_1 = val_1^1 \\wedge att_2 = val_1^2 \\wedge ...$ THEN $class = label_1$\n",
    "\n",
    ">IF $att_1 = val_2^1 \\wedge att_2 = val_2^2 \\wedge ...$ THEN $class = label_2$\n",
    "\n",
    ">IF $att_1 = val_3^1 \\wedge att_2 = val_3^2 \\wedge ...$ THEN $class = label_3$\n",
    "\n",
    "The rules are captured in a \"decision tree\"\n",
    "* Internal nodes denote attributes (e.g., job status, standing, etc.)\n",
    "* Edges denote values of the attribute\n",
    "* Leaves denote class labels (e.g., buys iphone = yes)\n",
    "    * Either stating a prediction\n",
    "    * Or giving the distribution...\n",
    "    \n",
    "### Lab Task 1\n",
    "An example for the iphone prediction example. iPhone Purchases (Fake) dataset:\n",
    "\n",
    "|standing |job_status |credit_rating |buys_iphone|\n",
    "|-|-|-|-|\n",
    "|1 |3 |fair |no|\n",
    "|1 |3 |excellent |no|\n",
    "|2 |3 |fair |yes|\n",
    "|2 |2 |fair |yes|\n",
    "|2 |1 |fair |yes|\n",
    "|2 |1 |excellent |no|\n",
    "|2 |1 |excellent |yes|\n",
    "|1 |2 |fair |no|\n",
    "|1 |1 |fair |yes|\n",
    "|2 |2 |fair |yes|\n",
    "|1 |2 |excellent |yes|\n",
    "|2 |2 |excellent |yes|\n",
    "|2 |3 |fair |yes|\n",
    "|2 |2 |excellent |no|\n",
    "|2 |3 |fair |yes|\n",
    "\n",
    "A *clash* is when two or more instances in a partition have the same combination of attribute values but different classifications. \n",
    "\n",
    "Bramer's definition of the Top-Down Induction of Decision Trees (TDIDT) assumes the *adequacy condition*, which ensures that no two instances with identical attribute values have different class labels (e.g. no clashes).\n",
    "\n",
    "Does the iPhone dataset have any clashes?\n",
    "\n",
    "\n",
    "### Lab Task 2\n",
    "Here is an example decision tree for the iPhone dataset:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/GonzagaCPSC322/U5-Decision-Trees/master/figures/iphone_decision_tree_example.png\" width=\"850\"/>\n",
    "\n",
    "* Attribute values are edges/links that represent \"partitions\" of the training set\n",
    "* Leaf nodes give distribution of class labels\n",
    "\n",
    "Extract the rules from this decision tree.\n",
    "\n",
    "### Lab Task 3\n",
    "Use the tree from the previous task to classify the following test instances:\n",
    "1. $X_{1}$ = `[standing = 2, job_status = 2, credit_rating = fair]`\n",
    "1. $X_{2}$ = `[standing = 1, job_status = 1, credit_rating = excellent]`\n",
    "\n",
    "How do these predictions compare to the predicted class labels from RQ5?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The TDIDT (Top-Down Induction of Decision Trees) Algorithm\n",
    "Basic Approach (uses recursion!):\n",
    "* At each step, pick an attribute (\"attribute selection\")\n",
    "* Partition data by attribute values ... this creates pairwise disjoint partitions\n",
    "* Repeat until one of the following occurs (base cases):\n",
    "    1. Partition has only class labels that are the same ... no clashes, make a leaf node\n",
    "    2. No more attributes to partition ... reached the end of a branch and there may be clashes, see options below\n",
    "    3. No more instances to partition ... see options below\n",
    "    \n",
    "### More on Case 3\n",
    "Assume we have the following:\n",
    "<img src=\"https://raw.githubusercontent.com/GonzagaCPSC322/U5-Decision-Trees/master/figures/decision_tree_one_attr.png\" width=\"300\"/>\n",
    "\n",
    "* Where the partition for att1=v1 has many instances\n",
    "* But the partition for att1=v2 has no instances\n",
    "* What are our options?\n",
    "    1. Do Nothing: Leave value out of tree (creates incomplete decision tree)\n",
    "    2. Backtrack: replace Attribute 1 node with leaf node (possibly w/clashes, see options below)\n",
    "* For the first choice, we won't be able to classify all instances\n",
    "* We also need to know the possible attribute values ahead of time\n",
    "\n",
    "### Handling Clashes for Prediction\n",
    "1. \"Majority Voting\"... select the class with highest number of instances\n",
    "    * On ties, \"flip a coin\"... which for ease of reproducibility could simply be choosing the first label alphabetically\n",
    "2. \"Intuition\"... that is, use common sense and pick one (hand modify tree)\n",
    "3. \"Discard\"... remove the branch from the node above\n",
    "    * Similar to case 3 above\n",
    "    * Results in \"missing\" attribute combos (some instances can't be classified)\n",
    "    * e.g., just remove two 50/50 branches from iPhone example tree\n",
    "    \n",
    "### Summary: TDIDT Algorithm (w/backtracking and majority voting)\n",
    "1. At each step, select an attribute to split on (“attribute selection” e.g. random, takefirst, takelast, entropy, gini, etc.)\n",
    "1. Group the data by attribute domain... (e.g. create pairwise disjoint partitions)\n",
    "1. For each partition, repeat unless one of the following occurs (base cases):\n",
    "    1. CASE 1: All class labels of the partition are the same (e.g. no clashes)\n",
    "        * => create a leaf node\n",
    "    1. CASE 2: No more attributes to split on (e.g. clash)\n",
    "        * => handle the clash with a majority vote leaf node\n",
    "    1. CASE 3: No more instances to partition (e.g. empty partition)\n",
    "        * => backtrack and replace subtree with majority vote leaf node\n",
    "\n",
    "Note: On majority vote ties, choose first label alphabetically (simplest for reproducibility)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab Task 4\n",
    "Use TDIDT to create a decision tree for iPhone example. Randomly select attributes as your \"attribute selection\" approach.\n",
    "\n",
    "|standing |job_status |credit_rating |buys_iphone|\n",
    "|-|-|-|-|\n",
    "|1 |3 |fair |no|\n",
    "|1 |3 |excellent |no|\n",
    "|2 |3 |fair |yes|\n",
    "|2 |2 |fair |yes|\n",
    "|2 |1 |fair |yes|\n",
    "|2 |1 |excellent |no|\n",
    "|2 |1 |excellent |yes|\n",
    "|1 |2 |fair |no|\n",
    "|1 |1 |fair |yes|\n",
    "|2 |2 |fair |yes|\n",
    "|1 |2 |excellent |yes|\n",
    "|2 |2 |excellent |yes|\n",
    "|2 |3 |fair |yes|\n",
    "|2 |2 |excellent |no|\n",
    "|2 |3 |fair |yes|\n",
    "\n",
    "### Lab Task 5\n",
    "Using the tree from the previous step, predict the class labels again for the following test instances:\n",
    "1. $X_{1}$ = `[standing = 2, job_status = 2, credit_rating = fair]`\n",
    "1. $X_{2}$ = `[standing = 1, job_status = 1, credit_rating = excellent]`"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
